# 第七周周报
### 学习了 多个变量特征归一化，了解了损失函数costFunction，梯度下降gradientDesent。  
### 可视化损失函数，比较不同alpha值的迭代效果
## 多变量线性回归。
### 读取数据，并进行特征归一化
```python
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
data=pd.read_csv('advertising.csv')
print(data.head())
def normalize_feature(data):
    return (data-data.mean())/data.std()
data=normalize_feature(data)
print(data.head())
```
![image](https://github.com/user-attachments/assets/e4c692bb-b8c5-40fe-9278-1c31690a98b5)  
### 显示不同平台投资对sales的影响散点图  
```python
data.plot.scatter('wechat','sales')
data.plot.scatter('weibo','sales')
data.plot.scatter('others','sales')
plt.show()
```
![Figure_1](https://github.com/user-attachments/assets/408baca5-8d8c-4695-b44b-b622ca3d20cc)
![Figure_2](https://github.com/user-attachments/assets/2a1cad06-c52d-4cd6-879d-7adebe1085f1)
![Figure_3](https://github.com/user-attachments/assets/434a93fa-a62c-4bfa-b143-28d274058e46)  
### 数据分割，定义损失函数与梯度下降。
```python
data.insert(0,'ones',1)
x=data.iloc[:,0:-1]
y=data.iloc[:,-1]
x=np.array(x)
y=np.array(y)
y=y.reshape(200,1)
print(x.shape,y.shape)
def costFunction(x,y,theta):
    inner=np.power(x@theta-y,2)
    return np.sum(inner)/(2*len(x))
theta=np.zeros((4,1))
cost_init=costFunction(x, y, theta)
print(cost_init)
def gradientDescent(x,y,theta,alpha,iters):
    costs=[]
    for i in range(iters):
        theta=theta-x.T@(x@theta-y)*alpha/len(x)
        cost=costFunction(x,y, theta)
        costs.append(cost)
    return theta,costs
```
### 比较不同alpha值对迭代效果的影响 
```python
candinate_alpha=[0.0002,0.002,0.02,0.0005,0.005,0.05]
iters=2000
fig,ax=plt.subplots()
for alpha in candinate_alpha:
    _,costs=gradientDescent(x, y, theta, alpha, iters)
    ax.plot(np.arange(iters),costs,label=alpha)
    ax.legend()
ax.set(xlabel='iters',ylabel='cost')
plt.show()
```
![Figure_4](https://github.com/user-attachments/assets/de3edafd-8e80-4d3f-8264-30eecadda433)  
### 多变量线性回归模型产出 
```python
alpha=0.02
iters=2000
theta,costs=gradientDescent(x,y,theta,alpha,iters)
x_=np.linspace(y.min(),y.max(),100)
print(theta)
y_=theta[0,0]+theta[1,0]*x_
plt.scatter(x[:,1],y,label='trainning data',c='b',s=16)
plt.plot(x_,y_,'r',label='predict')
plt.legend()
plt.show()
```
![Figure_5](https://github.com/user-attachments/assets/f68b918d-2796-4a31-831b-2634db6801c7)
